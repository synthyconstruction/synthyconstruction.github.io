<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="The Synthy Construction Dataset is a large-scale synthetic dataset for semantic segmentation of partial progress of indoor and outdoor construction elements.">
  <meta name="keywords" content="SConstruct">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Synthy Construction Dataset: A Synthetic Dataset for Construction Sites Indoor Environments</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Synthy Construction Dataset: A Synthetic Dataset for Construction Sites Indoor Environments</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="TBD">Author1</a><sup>1,3</sup>,</span>
            <span class="author-block">
              <a href="TBD">Author2</a><sup>2,3</sup>,</span>
          </div>
          
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Affiliation1,</span>
            <span class="author-block"><sup>2</sup>Affiliation2,</span>
            <span class="author-block"><sup>3</sup>Affiliation3</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="TBDD"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="TBD"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/drive/folders/1UbRDvvm0-zqk9GDbgrk1PFJF0KbY3BHA?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/background.png"/>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">SConsrtuct</span> contains a collection of indoor construction scenes, including several image modes.
      </h2>
    </div>
  </div>
</section>

  
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We present the first large-scale photorealistically synthetic dataset, the <span class="dnerf">Synthy Construction Dataset</span>,
            for indoor scenes of construction sites under active construction.
          </p>
          <p>
            The construction industry has continuously benefited from the advancements in computer vision 
            over the last decade. Despite the existing different data structures that any typical project 
            handles, vision-based information has created a wide range of commercial applications that have 
            varied from safety and hazard detections to optimization of construction activities and, more 
            recently, the comparison of as-built against plan documents. However, the repeatability of patterns, 
            numerous non-relevant objects visible in scenes, and significant occlusions make under-construction 
            scenes a significant challenge for scaling one-shot learning and unsupervised learning techniques. 
          </p>
          <p>
            To promote further advancements in computer vision applications for construction site monitoring, 
            we build on the extensive work done in supervised learning and introduce the <span class="dnerf">Synthy Construction Dataset</span> 
            , the largest supervised synthetic dataset for geometry-assisted semantic segmentation of 
            under-construction environments. Through this dataset, we introduce 1M high-quality perspective 
            images of indoor construction operations generated from a collection of 10 different Building 
            Information Models, consisting of RGB-D images with surface normals at 480x480 resolution. We 
            provide fully annotated scenes with up to 66 different classes that focus on states of work-in-progress 
            of relevant construction elements for applications in progress monitoring and detecting partial progress. 
          </p>
          
          <p>
            The effectiveness of our dataset is validated by training and testing several popular transformer 
            architecture encoders, reporting up to 30% improvements in metrics such as IoU, Accuracy, F1 Score, 
            Precision, and Recall. Through our extensive experiments and ablation studies, we demonstrate the 
            viability of using synthetic data from existing construction project documents as a reliable source 
            for learning robust visual features of construction scenes.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Photorealistic Images & Automated Image Modes</h2>
          <p>
            Using the <i>Synthy Construction</i> tool in <i>Unreal Engine</i>, you can collect your own photorealistic dataset for different
            scenes from your own BIM models. The higher the LOD of your model, the larger the variations. The pipeline completely 
            automates the data collection process, even for numerous image modes, including Surface Normals, Depthmaps, Lightmaps,
            and Segmentation Ground-Truth Masks.
          </p>
          <p>

          </p>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-data_collection_demo">
          <video poster="" id="data_collection_demo" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/data_collection_demo.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-surface_normal_demo">
          <video poster="" id="surface_normal_demo" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/surface_normal_demo.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-depthmaps_demo">
          <video poster="" id="depthmaps_demo" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/depthmaps_demo.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask_demo">
          <video poster="" id="mask_demo" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask_demo.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-random_light">
          <video poster="" id="random_light" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/random_light.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>


  
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      
      <!-- Pipeline Overview. -->
      <div class="column">
        <h2 class="title is-3"> </h2>
        <h2 class="title is-3">Pipeline Overview</h2>
        <div class="columns is-centered">
          <div class="column content">
            <img src="./static/images/pipeline_overview.png"/>
            <p>
               
            </p>
            <p>
              The developed pipeline uses BIM models as data input, python scripting for automating data collection tasks, 
              and Unreal Engine as the rendering platform. Starting with the BIM model as input, we can extract per-element 
              metadata such as GUID, name, type, level, material, and its Visual State of Work-in-Progress class. An 
              additional JSON structure output is created specifically for floor elements to create a grid-based 
              coordinate system to deploy different cameras for data collection automatically.

              Simultaneously, high-quality material textures, including their corresponding pixel values for their graphics 
              pipeline physics properties such as metallicity, specularity, and roughness, are automatically mapped against 
              corresponding BIM elements of their corresponding class. The aforementioned process is described as the Modeling 
              & Rendering stages of the tool.

              Next, different light objects are automatically created using Unreal Engine's object library. Each light 
              element follows the same grid-based coordinate system used for the cameras to be correctly deployed, using 
              a probabilistic threshold to determine which lights remain active during the data collection. In addition, 
              light intensities are modeled based on real-world intensity values that correspond to a clear, cloudy, and 
              overcast sky.

              With each component in place, the data simulation and collection process runs automatically, collecting 
              rendered images, depthmaps, surface normals, and ground truth segmentation masks. This process leverages 
              Unreal Engine's built-in visual modes and the same approach described above. Lastly, to generate ground 
              truth segmentation masks, per-class colors are mapped to each element in the same approach as material 
              textures are mapped to elements, and all built-in light models (e.g., Phong) are disabled to ensure the 
              collection of robust data masks.
            </p>
          </div>
        </div>
      </div>
    </div>
    <!--/ Pipeline Overview. -->
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      
      <!-- Sample Data. -->
      <div class="column">
        <h2 class="title is-3">Sample Data</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              At the moment, this dataset is collected from 11 high-LOD (Level of Development) Building Information Models 
              with different construction disciplines and contains 500,000 RGB-D images with corresponding surface normals 
              and ground-truth annotations for semantic segmentation of 90 construction-relevant objects and states-of-progress 
              classes based on the ASTM Uniformat II classification system. The relevance of the proposed classes lies in 
              enabling vision systems to detect the partial progress of different construction systems.
            </p>
            <img src="./static/images/data_sample.png"/>
          </div>
        </div>
      </div>
    </div>
    <!--/ Sample Data. -->
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      
      <!-- Semantic Segmentation Results. -->
      <div class="column">
        <h2 class="title is-3">Semantic Segmentation of Work-in-Progress (WIPs) Construction Classes</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              In our experiments, we employ transformer-based architectures, such as 
              <a href="https://arxiv.org/abs/2103.14030">Swin</a> to take advantage of 
              the global attention mechanisms. Moreover, due to the large-scale data 
              volume provided by this work, transformer architectures may learn better 
              global features than their convolution counterparts.
            </p>
            <img src="./static/images/segmentation_sample.png"/>
          </div>
        </div>
      </div>
    </div>
    <!--/ Semantic Segmentation Results. -->
</section>

  
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{synthyconstruction2024,
  author    = {Anonymous},
  title     = {Synthy Construction Dataset: A Visual Synthetic Dataset for the Under-Construction Environment},
  journal   = {TBD},
  year      = {2024},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/juandiegonm" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
