<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="The Synthy Construction Dataset is a large-scale synthetic dataset for semantic segmentation of partial progress of indoor and outdoor construction elements.">
  <meta name="keywords" content="SConstruct">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Synthy Construction Dataset: A Synthetic Visual Dataset for Construction Monitoring Applications in Indoor Environments</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Synthy Construction Dataset: A Synthetic Dataset for Construction Sites Indoor Environments</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="TBD">Author1</a><sup>1,3</sup>,</span>
            <span class="author-block">
              <a href="TBD">Author2</a><sup>2,3</sup>,</span>
          </div>
          
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Affiliation1,</span>
            <span class="author-block"><sup>2</sup>Affiliation2,</span>
            <span class="author-block"><sup>3</sup>Affiliation3</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/file/d/1DdN4AOzI6vfj20du9zymFOYQ9gk-UZjB/view?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/synthyconstruction/UnrealSynthyConstruction/tree/main"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/drive/folders/1UbRDvvm0-zqk9GDbgrk1PFJF0KbY3BHA?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
              <!-- Texture Library Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/file/d/1Nv9QoSbbqmQBRGruU42wrALcKhZAtc_Q/view?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
              <!-- Props Library Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/file/d/1_QmTYYq6XOOM7qXraER25SdOtnLrvYXZ/view?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/background.png"/>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Synthy Consrtuction</span> contains a collection of indoor construction scenes, including several image modes.
      </h2>
    </div>
  </div>
</section>

  
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We present the first large-scale photorealistically synthetic dataset, the <span class="dnerf">Synthy Construction Dataset</span>,
            for indoor scenes of construction sites under active construction.
          </p>
          <p>
            Over the last decade, many design, construction, operation, and maintenance applications in the 
            built environment have benefited from advancements in computer vision. However, scaling these 
            applications and their one-shot, supervised, and unsupervised learning techniques to real-world 
            environments --particularly those under construction-- has been a constant challenge. Specifically, 
            training underlying models need (1) ground truth for inter and intra-class variability of objects; 
            (2) variability in how images are taken; i.e., different camera locations, viewpoints, lighting, 
            and occlusions; and (3) availability of both appearance and geometrical ground truth. 
          </p>
          <p>
            To unlock the potential of these techniques, we build on the extensive work in supervised learning 
            and introduce the \textbf{Synthy Construction Dataset}, together with a procedural dataset generation 
            pipeline that leverages CAD models through gaming engines. Our dataset is the largest supervised 
            synthetic dataset for semantic segmentation of under-construction built environments, with collected 
            geometry information, ideal for 3D reconstruction tasks using SfM and MVS. We introduce 500k 
            high-quality perspective RGB-D images with surface normals at 480x480 resolution of indoor construction 
            environments generated from a collection of 10 different Building Information Models, consisting of 
            92 semantic classes of visual states of work-in-progress for building elements.
          </p>
          <p>
            The effectiveness of our dataset is validated by fine-tuning a transformer architecture encoder, 
            and tested on real data, reporting up to 30\% improvements in IoU, Accuracy, F1 Score, Precision, 
            and Recall. We also provide ablation studies comparing performances when working with other synthetic 
            datasets, and tested on other publicly available real datasets of the indoor scene environment, 
            demonstrating the viability of our dataset as a reliable source for learning robust visual features 
            of construction scenes.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Photorealistic Images & Automated Image Modes</h2>
          <p>
            Using the <i>Synthy Construction</i> tool in <i>Unreal Engine</i>, you can collect your own photorealistic dataset for different
            scenes from your own BIM models. The higher the LOD of your model, the larger the variations. The pipeline completely 
            automates the data collection process, even for numerous image modes, including Surface Normals, Depthmaps, Lightmaps,
            and Segmentation Ground-Truth Masks.
          </p>
          <p>

          </p>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-data_collection_demo">
          <video poster="" id="data_collection_demo" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/data_collection_demo.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-surface_normal_demo">
          <video poster="" id="surface_normal_demo" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/surface_normal_demo.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-depthmaps_demo">
          <video poster="" id="depthmaps_demo" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/depthmaps_demo.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask_demo">
          <video poster="" id="mask_demo" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask_demo.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-random_light">
          <video poster="" id="random_light" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/random_light.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>


  
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      
      <!-- Pipeline Overview. -->
      <div class="column">
        <h2 class="title is-3"> </h2>
        <h2 class="title is-3">Pipeline Overview</h2>
        <div class="columns is-centered">
          <div class="column content">
            <img src="./static/images/pipeline_overview.png"/>
            <p>
               
            </p>
            <p>
              The developed pipeline uses BIM models as data input, python scripting for automating data collection tasks, 
              and Unreal Engine as the rendering platform. Starting with the BIM model as input, we can extract per-element 
              metadata such as GUID, name, type, level, material, and its Visual State of Work-in-Progress class. An 
              additional JSON structure output is created specifically for floor elements to create a grid-based 
              coordinate system to deploy different cameras for data collection automatically.

              Simultaneously, high-quality material textures, including their corresponding pixel values for their graphics 
              pipeline physics properties such as metallicity, specularity, and roughness, are automatically mapped against 
              corresponding BIM elements of their corresponding class. The aforementioned process is described as the Modeling 
              & Rendering stages of the tool.

              Next, different light objects are automatically created using Unreal Engine's object library. Each light 
              element follows the same grid-based coordinate system used for the cameras to be correctly deployed, using 
              a probabilistic threshold to determine which lights remain active during the data collection. In addition, 
              light intensities are modeled based on real-world intensity values that correspond to a clear, cloudy, and 
              overcast sky.

              With each component in place, the data simulation and collection process runs automatically, collecting 
              rendered images, depthmaps, surface normals, and ground truth segmentation masks. This process leverages 
              Unreal Engine's built-in visual modes and the same approach described above. Lastly, to generate ground 
              truth segmentation masks, per-class colors are mapped to each element in the same approach as material 
              textures are mapped to elements, and all built-in light models (e.g., Phong) are disabled to ensure the 
              collection of robust data masks.
            </p>
          </div>
        </div>
      </div>
    </div>
    <!--/ Pipeline Overview. -->
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      
      <!-- Sample Data. -->
      <div class="column">
        <h2 class="title is-3">Sample Data</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              At the moment, this dataset is collected from 11 high-LOD (Level of Development) Building Information Models 
              with different construction disciplines and contains 500,000 RGB-D images with corresponding surface normals 
              and ground-truth annotations for semantic segmentation of 90 construction-relevant objects and states-of-progress 
              classes based on the ASTM Uniformat II classification system. The relevance of the proposed classes lies in 
              enabling vision systems to detect the partial progress of different construction systems.
            </p>
            <img src="./static/images/data_sample.png"/>
          </div>
        </div>
      </div>
    </div>
    <!--/ Sample Data. -->
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      
      <!-- Semantic Segmentation Results. -->
      <div class="column">
        <h2 class="title is-3">Semantic Segmentation of Work-in-Progress (WIPs) Construction Classes</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              In our experiments, we employ transformer-based architectures, such as 
              <a href="https://arxiv.org/abs/2103.14030">Swin</a> to take advantage of 
              the global attention mechanisms. Moreover, due to the large-scale data 
              volume provided by this work, transformer architectures may learn better 
              global features than their convolution counterparts.
            </p>
            <img src="./static/images/segmentation_sample.png"/>
          </div>
        </div>
      </div>
    </div>
    <!--/ Semantic Segmentation Results. -->
</section>

  
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{synthyconstruction2024,
  author    = {Anonymous},
  title     = {Synthy Construction Dataset: A Synthetic Visual Dataset for Construction Monitoring Applications in Indoor Environments},
  journal   = {TBD},
  year      = {2024},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://drive.google.com/file/d/1DdN4AOzI6vfj20du9zymFOYQ9gk-UZjB/view?usp=sharing">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/synthyconstruction" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
